{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NAMED ENTITY RECOGNITION (NER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import svgling\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading conll2002: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "\n",
    "# Spanish\n",
    "train_esp = conll2002.iob_sents('esp.train') # Train\n",
    "val_esp = conll2002.iob_sents('esp.testa') # Val\n",
    "test_esp = conll2002.iob_sents('esp.testb') # Test\n",
    "# Dutch\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "val_ned = conll2002.iob_sents('ned.testa') # Val\n",
    "test_ned = conll2002.iob_sents('ned.testb') # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from class_feature_getter import FeatureGetter\n",
    "\n",
    "feature_getter = FeatureGetter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_esp = nltk.tag.CRFTagger()\n",
    "tagger_ned = nltk.tag.CRFTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuples(X : list) -> list:\n",
    "    new = []\n",
    "    for s in X:\n",
    "        t = []\n",
    "        for w in s:\n",
    "            t.append((w[0], w[2]))\n",
    "        new.append(t)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_esp_tuples = get_tuples(train_esp)\n",
    "train_ned_tuples = get_tuples(train_ned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_esp_tuples = get_tuples(test_esp)\n",
    "test_ned_tuples = get_tuples(test_ned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('De', 'O'),\n",
       " ('tekst', 'O'),\n",
       " ('van', 'O'),\n",
       " ('het', 'O'),\n",
       " ('arrest', 'O'),\n",
       " ('is', 'O'),\n",
       " ('nog', 'O'),\n",
       " ('niet', 'O'),\n",
       " ('schriftelijk', 'O'),\n",
       " ('beschikbaar', 'O'),\n",
       " ('maar', 'O'),\n",
       " ('het', 'O'),\n",
       " ('bericht', 'O'),\n",
       " ('werd', 'O'),\n",
       " ('alvast', 'O'),\n",
       " ('bekendgemaakt', 'O'),\n",
       " ('door', 'O'),\n",
       " ('een', 'O'),\n",
       " ('communicatiebureau', 'O'),\n",
       " ('dat', 'O'),\n",
       " ('Floralux', 'B-ORG'),\n",
       " ('inhuurde', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ned_tuples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_esp.train(train_esp_tuples, \"./esp_model.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_ned.train(train_ned_tuples, \"./ned_model.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9622960045019696"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger_esp.accuracy(test_esp_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codi del profe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  2519\n",
      "FN:  1039\n",
      "FP:  879\n",
      "TOT 4437\n",
      "TP:  2449\n",
      "FN:  1492\n",
      "FP:  1045\n",
      "TOT 4986\n"
     ]
    }
   ],
   "source": [
    "test_data_es_tolist = [[token for token, _ in sentence] for sentence in test_esp_tuples]\n",
    "test_data_nl_tolist = [[token for token, _ in sentence] for sentence in test_ned_tuples]\n",
    "\n",
    "\n",
    "from typing import List, Tuple, Set, Any\n",
    "\n",
    "def decode_entities(phrase: List[Tuple[Any, str]]) -> Set[Tuple[int, int, str]]:\n",
    "    first_idx = -1\n",
    "    current_entity = None\n",
    "    \n",
    "    result = set()\n",
    "    for i, (token, label) in enumerate(phrase):\n",
    "        if label[0] == \"B\" or label == \"O\":\n",
    "            if current_entity:\n",
    "                result.add((first_idx, i, current_entity))\n",
    "                current_entity = None\n",
    "            if label[0] == \"B\":\n",
    "                first_idx = i\n",
    "                current_entity = label[2:]\n",
    "    if current_entity:\n",
    "        result.add((first_idx, len(phrase), current_entity))\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "tagged_es = []\n",
    "for sentence in test_data_es_tolist:\n",
    "    tagged_sentence = tagger_esp.tag(sentence)\n",
    "    tagged_es.append(tagged_sentence)\n",
    "    #decoded_es.append(decode_entities(tagged_sentence))\n",
    "\n",
    "tagged_nl = []\n",
    "for sentence in test_data_nl_tolist:\n",
    "    tagged_sentence = tagger_ned.tag(sentence)\n",
    "    tagged_nl.append(tagged_sentence)\n",
    "    #decoded_nl.append(decode_entities(tagged_sentence))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def evaluate(gold: List[List[Tuple[Any, str]]], predicted: List[List[Tuple[Any, str]]]) -> Tuple[int, int, int]:\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    tot = 0\n",
    "    for gold_sentence, predicted_sentence in zip(gold, predicted):\n",
    "        #gold_sentence, predicted_sentence = eliminate_rest(gold_sentence), eliminate_rest(predicted_sentence)\n",
    "        gold_entities = decode_entities(gold_sentence)\n",
    "        predicted_entities = decode_entities(predicted_sentence)\n",
    "        tp += len(gold_entities.intersection(predicted_entities))\n",
    "        fn += len(gold_entities.difference(predicted_entities))\n",
    "        fp += len(predicted_entities.difference(gold_entities))\n",
    "        tot += len(gold_entities.union(predicted_entities))\n",
    "        '''\n",
    "        if gold_entities != predicted_entities:\n",
    "            print(\"GOLD sentence: \", gold_sentence)\n",
    "            print(\"PRED sentence: \", predicted_sentence)\n",
    "            for i in range(len(gold_sentence)):\n",
    "                if gold_sentence[i][1] != predicted_sentence[i][1]:\n",
    "                    print(f\"ERROR {i} --- Gold: {gold_sentence[i]} Predicted: {predicted_sentence[i]}\")\n",
    "        #'''\n",
    "    print(\"TP: \", tp)\n",
    "    print(\"FN: \", fn)\n",
    "    print(\"FP: \", fp)\n",
    "    print(\"TOT\", tot)\n",
    "    \n",
    "    return tp, fn, fp\n",
    "\n",
    "\n",
    "\n",
    "tp_es, fn_es, fp_es = evaluate(test_esp_tuples, tagged_es)\n",
    "tp_nl, fn_nl, fp_nl = evaluate(test_ned_tuples, tagged_nl)\n",
    "\n",
    "tp_es, fn_es, fp_es = tp_es / (tp_es + fn_es), tp_es / (tp_es + fp_es), 2 * tp_es / (2 * tp_es + fn_es + fp_es)\n",
    "tp_nl, fn_nl, fp_nl = tp_nl / (tp_nl + fn_nl), tp_nl / (tp_nl + fp_nl), 2 * tp_nl / (2 * tp_nl + fn_nl + fp_nl)\n",
    "\n",
    "tp_es_f1 = 2 * tp_es / (2 * tp_es + fn_es + fp_es)\n",
    "tp_nl_f1 = 2 * tp_nl / (2 * tp_nl + fn_nl + fp_nl)\n",
    "\n",
    "recall_es = tp_es / (tp_es + fn_es)\n",
    "precision_es = tp_es / (tp_es + fp_es)\n",
    "\n",
    "recall_nl = tp_nl / (tp_nl + fn_nl)\n",
    "precision_nl = tp_nl / (tp_nl + fp_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5677259409510931"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2519 / 4437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49117529081427996"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2449 / 4986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4943149516770893"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.485408369785206"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": " No model file is found !! Please use train or set_model_file function",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tagged_text_esp \u001b[38;5;241m=\u001b[39m \u001b[43mtagger_esp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords_esp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m tagged_text_ned \u001b[38;5;241m=\u001b[39m tagger_ned\u001b[38;5;241m.\u001b[39mtag(words_ned)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Universitat\\4rt Quatri\\PLH\\Practica3-NER\\named-entity-recognition\\.venv\\Lib\\site-packages\\nltk\\tag\\crf.py:207\u001b[0m, in \u001b[0;36mCRFTagger.tag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtag\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens):\n\u001b[0;32m    195\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m    Tag a sentence using Python CRFSuite Tagger. NB before using this function, user should specify the mode_file either by\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str,str))\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtag_sents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Universitat\\4rt Quatri\\PLH\\Practica3-NER\\named-entity-recognition\\.venv\\Lib\\site-packages\\nltk\\tag\\crf.py:155\u001b[0m, in \u001b[0;36mCRFTagger.tag_sents\u001b[1;34m(self, sents)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03mTag a list of sentences. NB before using this function, user should specify the mode_file either by\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;124;03m:rtype: list(list(tuple(str,str)))\u001b[39;00m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_file \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m No model file is found !! Please use train or set_model_file function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# We need the list of sentences instead of the list generator for matching the input and output\u001b[39;00m\n\u001b[0;32m    160\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mException\u001b[0m:  No model file is found !! Please use train or set_model_file function"
     ]
    }
   ],
   "source": [
    "tagged_text_esp = tagger_esp.tag(words_esp)\n",
    "tagged_text_ned = tagger_ned.tag(words_ned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
