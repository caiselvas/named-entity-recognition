{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **NAMED ENTITY RECOGNITION (NER)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -r requirements.txt\n",
    "!python -m spacy download es_core_news_sm\n",
    "!python -m spacy download nl_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import svgling\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "\n",
    "# Spanish\n",
    "train_esp = conll2002.iob_sents('esp.train') # Train\n",
    "val_esp = conll2002.iob_sents('esp.testa') # Val\n",
    "test_esp = conll2002.iob_sents('esp.testb') # Test\n",
    "# Dutch\n",
    "train_ned = conll2002.iob_sents('ned.train') # Train\n",
    "val_ned = conll2002.iob_sents('ned.testa') # Val\n",
    "test_ned = conll2002.iob_sents('ned.testb') # Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mycrftagger_class import MyCRFTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_esp = MyCRFTagger(verbose=True, language=\"esp\")\n",
    "tagger_ned = MyCRFTagger(verbose=True, language=\"ned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuples(X : list) -> list:\n",
    "    new = []\n",
    "    for s in X:\n",
    "        t = []\n",
    "        for w in s:\n",
    "            t.append((w[0], w[2]))\n",
    "        new.append(t)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_esp_tuples = get_tuples(train_esp)\n",
    "train_ned_tuples = get_tuples(train_ned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_esp_tuples = get_tuples(test_esp)\n",
    "test_ned_tuples = get_tuples(test_ned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('De', 'O'),\n",
       " ('tekst', 'O'),\n",
       " ('van', 'O'),\n",
       " ('het', 'O'),\n",
       " ('arrest', 'O'),\n",
       " ('is', 'O'),\n",
       " ('nog', 'O'),\n",
       " ('niet', 'O'),\n",
       " ('schriftelijk', 'O'),\n",
       " ('beschikbaar', 'O'),\n",
       " ('maar', 'O'),\n",
       " ('het', 'O'),\n",
       " ('bericht', 'O'),\n",
       " ('werd', 'O'),\n",
       " ('alvast', 'O'),\n",
       " ('bekendgemaakt', 'O'),\n",
       " ('door', 'O'),\n",
       " ('een', 'O'),\n",
       " ('communicatiebureau', 'O'),\n",
       " ('dat', 'O'),\n",
       " ('Floralux', 'B-ORG'),\n",
       " ('inhuurde', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ned_tuples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Currently, NLTK pos_tag only supports English and Russian (i.e. lang='eng' or lang='rus')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtagger_esp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_esp_tuples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./esp_model.mdl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Universitat\\4rt Quatri\\PLH\\Practica3-NER\\named-entity-recognition\\mycrftagger_class.py:303\u001b[0m, in \u001b[0;36mMyCRFTagger.train\u001b[1;34m(self, train_data, model_file)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[0;32m    302\u001b[0m \ttokens, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39msent)\n\u001b[1;32m--> 303\u001b[0m \tfeatures \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    304\u001b[0m \ttrainer\u001b[38;5;241m.\u001b[39mappend(features, labels)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;66;03m# Now train the model, the output should be model_file\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Universitat\\4rt Quatri\\PLH\\Practica3-NER\\named-entity-recognition\\mycrftagger_class.py:303\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m train_data:\n\u001b[0;32m    302\u001b[0m \ttokens, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39msent)\n\u001b[1;32m--> 303\u001b[0m \tfeatures \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(tokens))]\n\u001b[0;32m    304\u001b[0m \ttrainer\u001b[38;5;241m.\u001b[39mappend(features, labels)\n\u001b[0;32m    306\u001b[0m \u001b[38;5;66;03m# Now train the model, the output should be model_file\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Universitat\\4rt Quatri\\PLH\\Practica3-NER\\named-entity-recognition\\mycrftagger_class.py:205\u001b[0m, in \u001b[0;36mMyCRFTagger._get_features\u001b[1;34m(self, tokens, idx)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    203\u001b[0m \tfeature_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNEXT_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m tokens[idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m--> 205\u001b[0m pos_tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_postag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# POS tag the sentence\u001b[39;00m\n\u001b[0;32m    207\u001b[0m feature_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOS_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m pos_tags[idx][\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Universitat\\4rt Quatri\\PLH\\Practica3-NER\\named-entity-recognition\\mycrftagger_class.py:127\u001b[0m, in \u001b[0;36mMyCRFTagger.get_postag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;129m@cache\u001b[39m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_postag\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m--> 127\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pos_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_language\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Universitat\\4rt Quatri\\PLH\\Practica3-NER\\named-entity-recognition\\.venv\\Lib\\site-packages\\nltk\\tag\\__init__.py:166\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03mUse NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03mtag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m:rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    165\u001b[0m tagger \u001b[38;5;241m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtagger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Universitat\\4rt Quatri\\PLH\\Practica3-NER\\named-entity-recognition\\.venv\\Lib\\site-packages\\nltk\\tag\\__init__.py:114\u001b[0m, in \u001b[0;36m_pos_tag\u001b[1;34m(tokens, tagset, tagger, lang)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_pos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tagger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# Currently only supports English and Russian.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrus\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    115\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently, NLTK pos_tag only supports English and Russian \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(i.e. lang=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or lang=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrus\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m         )\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# Throws Error if tokens is of string type\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tokens, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Currently, NLTK pos_tag only supports English and Russian (i.e. lang='eng' or lang='rus')"
     ]
    }
   ],
   "source": [
    "tagger_esp.train(train_esp_tuples, \"./esp_model.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtagger_ned\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ned_tuples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./ned_model.mdl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Universitat\\4rt Quatri\\PLH\\Practica3-NER\\named-entity-recognition\\mycrftagger_class.py:284\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(self, train_data, model_file)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_data, model_file):\n\u001b[0;32m    280\u001b[0m \u001b[38;5;250m\t\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m\tTrain the CRF tagger using CRFSuite\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m\t:params train_data : is the list of annotated sentences.\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m\t:type train_data : list (list(tuple(str,str)))\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;124;03m\t:params model_file : the model will be saved to this file.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \n\u001b[0;32m    286\u001b[0m \u001b[38;5;124;03m\t\"\"\"\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \ttrainer \u001b[38;5;241m=\u001b[39m pycrfsuite\u001b[38;5;241m.\u001b[39mTrainer(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose)\n\u001b[0;32m    288\u001b[0m \ttrainer\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_options)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Universitat\\4rt Quatri\\PLH\\Practica3-NER\\named-entity-recognition\\mycrftagger_class.py:284\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_data, model_file):\n\u001b[0;32m    280\u001b[0m \u001b[38;5;250m\t\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m\tTrain the CRF tagger using CRFSuite\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m\t:params train_data : is the list of annotated sentences.\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m\t:type train_data : list (list(tuple(str,str)))\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;124;03m\t:params model_file : the model will be saved to this file.\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \n\u001b[0;32m    286\u001b[0m \u001b[38;5;124;03m\t\"\"\"\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \ttrainer \u001b[38;5;241m=\u001b[39m pycrfsuite\u001b[38;5;241m.\u001b[39mTrainer(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose)\n\u001b[0;32m    288\u001b[0m \ttrainer\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_training_options)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Documents\\Universitat\\4rt Quatri\\PLH\\Practica3-NER\\named-entity-recognition\\mycrftagger_class.py:130\u001b[0m, in \u001b[0;36m_get_features\u001b[1;34m(self, tokens, idx)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;129m@cache\u001b[39m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_postag\u001b[39m(\u001b[38;5;28mself\u001b[39m, tokens) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    128\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos_tagger(tokens)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;129m@cache\u001b[39m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_in_names\u001b[39m(\u001b[38;5;28mself\u001b[39m, token):\n\u001b[0;32m    132\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_names\n\u001b[0;32m    134\u001b[0m \u001b[38;5;129m@cache\u001b[39m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_in_surnames\u001b[39m(\u001b[38;5;28mself\u001b[39m, token):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tagger_ned.train(train_ned_tuples, \"./ned_model.mdl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codi del profe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_esp = MyCRFTagger(verbose=True, language=\"esp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_esp.set_model_file(\"./esp_model.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9589389323346206"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tagger_esp.accuracy(test_esp_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  2519\n",
      "FN:  1039\n",
      "FP:  879\n",
      "TOT 4437\n",
      "TP:  2449\n",
      "FN:  1492\n",
      "FP:  1045\n",
      "TOT 4986\n"
     ]
    }
   ],
   "source": [
    "test_data_es_tolist = [[token for token, _ in sentence] for sentence in test_esp_tuples]\n",
    "test_data_nl_tolist = [[token for token, _ in sentence] for sentence in test_ned_tuples]\n",
    "\n",
    "\n",
    "from typing import List, Tuple, Set, Any\n",
    "\n",
    "def decode_entities(phrase: List[Tuple[Any, str]]) -> Set[Tuple[int, int, str]]:\n",
    "    first_idx = -1\n",
    "    current_entity = None\n",
    "    \n",
    "    result = set()\n",
    "    for i, (token, label) in enumerate(phrase):\n",
    "        if label[0] == \"B\" or label == \"O\":\n",
    "            if current_entity:\n",
    "                result.add((first_idx, i, current_entity))\n",
    "                current_entity = None\n",
    "            if label[0] == \"B\":\n",
    "                first_idx = i\n",
    "                current_entity = label[2:]\n",
    "    if current_entity:\n",
    "        result.add((first_idx, len(phrase), current_entity))\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "tagged_es = []\n",
    "for sentence in test_data_es_tolist:\n",
    "    tagged_sentence = tagger_esp.tag(sentence)\n",
    "    tagged_es.append(tagged_sentence)\n",
    "    #decoded_es.append(decode_entities(tagged_sentence))\n",
    "\n",
    "tagged_nl = []\n",
    "for sentence in test_data_nl_tolist:\n",
    "    tagged_sentence = tagger_ned.tag(sentence)\n",
    "    tagged_nl.append(tagged_sentence)\n",
    "    #decoded_nl.append(decode_entities(tagged_sentence))\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def evaluate(gold: List[List[Tuple[Any, str]]], predicted: List[List[Tuple[Any, str]]]) -> Tuple[int, int, int]:\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    fp = 0\n",
    "    tot = 0\n",
    "    for gold_sentence, predicted_sentence in zip(gold, predicted):\n",
    "        #gold_sentence, predicted_sentence = eliminate_rest(gold_sentence), eliminate_rest(predicted_sentence)\n",
    "        gold_entities = decode_entities(gold_sentence)\n",
    "        predicted_entities = decode_entities(predicted_sentence)\n",
    "        tp += len(gold_entities.intersection(predicted_entities))\n",
    "        fn += len(gold_entities.difference(predicted_entities))\n",
    "        fp += len(predicted_entities.difference(gold_entities))\n",
    "        tot += len(gold_entities.union(predicted_entities))\n",
    "        '''\n",
    "        if gold_entities != predicted_entities:\n",
    "            print(\"GOLD sentence: \", gold_sentence)\n",
    "            print(\"PRED sentence: \", predicted_sentence)\n",
    "            for i in range(len(gold_sentence)):\n",
    "                if gold_sentence[i][1] != predicted_sentence[i][1]:\n",
    "                    print(f\"ERROR {i} --- Gold: {gold_sentence[i]} Predicted: {predicted_sentence[i]}\")\n",
    "        #'''\n",
    "    print(\"TP: \", tp)\n",
    "    print(\"FN: \", fn)\n",
    "    print(\"FP: \", fp)\n",
    "    print(\"TOT\", tot)\n",
    "    \n",
    "    return tp, fn, fp\n",
    "\n",
    "\n",
    "\n",
    "tp_es, fn_es, fp_es = evaluate(test_esp_tuples, tagged_es)\n",
    "tp_nl, fn_nl, fp_nl = evaluate(test_ned_tuples, tagged_nl)\n",
    "\n",
    "tp_es, fn_es, fp_es = tp_es / (tp_es + fn_es), tp_es / (tp_es + fp_es), 2 * tp_es / (2 * tp_es + fn_es + fp_es)\n",
    "tp_nl, fn_nl, fp_nl = tp_nl / (tp_nl + fn_nl), tp_nl / (tp_nl + fp_nl), 2 * tp_nl / (2 * tp_nl + fn_nl + fp_nl)\n",
    "\n",
    "tp_es_f1 = 2 * tp_es / (2 * tp_es + fn_es + fp_es)\n",
    "tp_nl_f1 = 2 * tp_nl / (2 * tp_nl + fn_nl + fp_nl)\n",
    "\n",
    "recall_es = tp_es / (tp_es + fn_es)\n",
    "precision_es = tp_es / (tp_es + fp_es)\n",
    "\n",
    "recall_nl = tp_nl / (tp_nl + fn_nl)\n",
    "precision_nl = tp_nl / (tp_nl + fp_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5677259409510931"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2519 / 4437"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49117529081427996"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2449 / 4986"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4943149516770893"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.485408369785206"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words_esp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tagged_text_esp \u001b[38;5;241m=\u001b[39m tagger_esp\u001b[38;5;241m.\u001b[39mtag(\u001b[43mwords_esp\u001b[49m)\n\u001b[0;32m      2\u001b[0m tagged_text_ned \u001b[38;5;241m=\u001b[39m tagger_ned\u001b[38;5;241m.\u001b[39mtag(words_ned)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'words_esp' is not defined"
     ]
    }
   ],
   "source": [
    "tagged_text_esp = tagger_esp.tag(words_esp)\n",
    "tagged_text_ned = tagger_ned.tag(words_ned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
